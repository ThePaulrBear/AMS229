{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming convex optimization problems$ \n",
    "% Text formating\n",
    "\\newcommand {\\th}{^{th}} % for ith, jth, etc. \n",
    "% General Latex Definitions\n",
    "\\newcommand {\\vec}{\\mathbf} % Vector variable\n",
    "\\newcommand {\\mat}{} % Matrix variable\n",
    "\\newcommand {\\set}{\\mathcal} % Set name\n",
    "\\newcommand{\\invs}{^{-1}}\n",
    "\\newcommand{\\trans}{^ \\top} % Vector or matrix transpose notation (\\intercal is an alternative)\n",
    "\\newcommand{\\suchthat}{\\mid} %Alternative: \\mathrel{} \\middle| \\mathrel{} \n",
    "\\newcommand {\\definedas}{:=}\n",
    "\\newcommand {\\arctan}{\\text{tan}^{-1}}\n",
    "\\newcommand {\\abs}[1]{|#1|}\n",
    "\\newcommand {\\for}{\\text{for}\\;}\n",
    "\\newcommand {\\and}{\\quad \\text{and}\\quad}\n",
    "\\newcommand {\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand {\\onenorm}[1]{\\norm{#1}_{1}}\n",
    "\\newcommand {\\twonorm}[1]{\\norm{#1}_{2}}\n",
    "\\newcommand {\\pnorm}[1]{\\norm{#1}_{p}}\n",
    "\\newcommand {\\inftynorm}[1]{\\norm{#1}_{\\infty}}\n",
    "% Sets\n",
    "\\newcommand {\\reals}{\\mathbb{R}}\n",
    "\\newcommand {\\realsn}{\\reals^{n}}\n",
    "\\newcommand {\\positivereals}{\\reals_{>0}}\n",
    "\\newcommand {\\integers}{\\mathbb{Z}}\n",
    "\\newcommand \\squarematrices[1][n]{\\reals^{#1 \\times #1}}\n",
    "\\newcommand \\symmetricmatrices[1][n]{\\mathbb{S}^{#1}}\n",
    "\\newcommand \\pdmatrices[1][n]{\\symmetricmatrices[#1]_{++}}\n",
    "\\newcommand \\psdmatrices[1][n]{\\symmetricmatrices[#1]_{+}}\n",
    "% Common vectors. If a value is a variable don't use the \"\\\"\n",
    "\\newcommand {\\x}{\\vec x}\n",
    "\\newcommand {\\y}{\\vec y} \n",
    "\\newcommand {\\u}{\\vec u}\n",
    "\\newcommand {\\v}{\\vec v}\n",
    "\\newcommand {\\onevec}{\\mathbb{1}}\n",
    "% Common Matrices\n",
    "\\newcommand {\\A}{\\mat A} \n",
    "\\newcommand {\\B}{\\mat B}\n",
    "\\newcommand {\\X}{\\mat X} \n",
    "\\newcommand {\\Y}{\\mat Y}\n",
    "\\newcommand {\\I}{\\mat I} % Identity\n",
    "% Matrix shortcut\n",
    "\\newcommand {\\beginmatrix}{\\begin{bmatrix}}\n",
    "\\newcommand {\\endmatrix}{\\end{bmatrix}}\n",
    "\\newcommand {\\beginalign}{\\begin{align}}\n",
    "\\newcommand {\\endalign}{\\end{align}}\n",
    "% Calculus\n",
    "\\newcommand {\\derive}[2]{\\frac{d#1}{d#2}}\n",
    "\\newcommand {\\ddx}{\\derive{}{x}}\n",
    "\\newcommand {\\ddt}{\\derive{}{t}}\n",
    "\\newcommand {\\dxdt}{\\derive{x}{t}}\n",
    "\\newcommand {\\dydt}{\\derive{y}{t}}\n",
    "\\newcommand {\\dfdx}{\\derive{f}{x}}\n",
    "\\newcommand {\\del}{\\nabla}\n",
    "\\newcommand {\\hessian}{\\del^2}\n",
    "% Convex Optimizations\n",
    "\\newcommand \\convexcombo[2]{\\theta #1 + (1 - \\theta)#2}\n",
    "\\newcommand \\minimize[1]{\\underset{#1}{\\text{minimize}}\\quad} % Usage: \\minimize{\\x \\in \\reals}\n",
    "\\newcommand {\\subjectto}{\\text{subject to}\\quad}\n",
    "% Exponent Taylor Series Definition\n",
    "\\newcommand \\exponentsum[1]{\\sum^\\infty_{k=1} \\frac{#1^k}{k!}}\n",
    "% Matrix shortcuts\n",
    "\\newcommand \\diagmatrix[2]{\n",
    "\\begin{bmatrix} \n",
    "#1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & #2 \\\\\n",
    "\\end{bmatrix}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## (a) [10 points] LP as special case of SDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $A\\in\\reals^{m\\times n}$, $b\\in\\reals^{m}$, $c\\in\\realsn$, rewrite the LP\n",
    "\n",
    "$$\\minimize{x\\in\\realsn} c^{\\top}x\\\\\n",
    "\\text{subject to}\\quad Ax \\preceq b,$$\n",
    "\n",
    "as the SDP\n",
    "\n",
    "$$\\underset{x\\in\\mathbb{R}^{n}}{\\text{minimize}}\\quad c^{\\top}x\\\\\n",
    "\\text{subject to}\\quad F(x) \\succeq 0,$$\n",
    "\n",
    "that is, write the matrix $F(x)$ appearing in the LMI constraint of the SDP in terms of the data of the LP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\vec a_i$ be the $i\\th$ column vector of $A$ and $(\\vec a_i)_j$ be the $j\\th$ component of $\\vec a_i$. \n",
    "\n",
    "The inequality $A\\x \\preceq \\vec b$ is equivalent to $\\vec a_i \\trans \\x   - b_i \\leq 0 $, so $b_i - \\vec a_i \\trans \\x \\geq 0, \\quad \\forall i \\in 1, \\cdots, n$. \n",
    "\n",
    "Let us now take the following diagonal matrix, which must be positive semidefinite, because the diagonal values, which are the eigenvalues, are all nonnegative. \n",
    "\n",
    "$$\\diagmatrix{b_1-\\vec a_1\\trans \\x}{b_n-\\vec a_n\\trans \\x}$$\n",
    "\n",
    "Thus, we can rewrite the inequality constraint as\n",
    "\n",
    "$$F(\\x) = \\diagmatrix{b_1}{b_n} + \\sum_{i=1}^{n} x_i \\diagmatrix{-(\\vec a_1)_i}{-(\\vec a_n)_i} \\succeq 0$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## (b) [10 points] Second-order Cone Programming as special case of Semi-Definite Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given $\\vec f\\in\\realsn$, $A_i\\in\\reals^{(n_i-1)\\times n}$, $\\vec b_i\\in\\reals^{n_i-1}$, $c_{i}\\in\\reals^{n}$, $d_i\\in\\reals$, rewrite the SOCP \n",
    "\n",
    "$$\\minimize{x\\in\\realsn} \\vec f^{\\top} \\x \\\\\n",
    "\\subjectto \\twonorm{A_i \\x + \\vec b_i} \\;\\leq\\; \\vec c_i\\trans \\x + d_i, \\quad i=1,...,m,$$\n",
    "\n",
    "as the SDP\n",
    "\n",
    "$$\\minimize{x\\in\\realsn} \\vec f\\trans \\x \\\\\n",
    "\\subjectto F(x) \\succeq 0,$$\n",
    "\n",
    "that is, write the matrix $F(x)$ appearing in the LMI constraint of the SDP in terms of the data of the SOCP.\n",
    "\n",
    "(Hint: Use the Schur Complement Lemma in Lecture 8, p. 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### First, show $\\twonorm {\\x}^2 \\leq t^2$ is convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_Proof:_\n",
    "$$\\beginalign \n",
    "& \\twonorm {\\x} \\leq t \\\\\n",
    "\\implies &\\twonorm{\\x}^2 \\leq t^2 \\\\\n",
    "\\implies &\\frac{1}{t} \\twonorm{\\x}^2 \\leq t \\\\\n",
    "\\implies &t - \\frac{1}{t} \\twonorm{\\x}^2 \\geq 0\n",
    "\\endalign$$\n",
    "\n",
    "We now note that \n",
    "$$\\beginalign  \n",
    "t - \\frac{1}{t} \\twonorm \\x ^2 \n",
    "&= t - \\x\\trans \\frac{1}{t} I \\x \\\\\n",
    "&= t - \\x\\trans (tI)\\invs \\x \\\\\n",
    "&= C - B\\trans A\\invs B \\\\\n",
    "&= S\n",
    "\\endalign $$\n",
    "\n",
    "Where $S = t - \\x\\trans (tI)\\invs \\x $ is the Schar complement of the the matrix.\n",
    "\n",
    "$$ X = \\beginmatrix \n",
    "A & B \\\\ B\\trans & C\n",
    "\\endmatrix\n",
    "=\n",
    "\\beginmatrix \n",
    "tI & \\x \\\\ \\x\\trans & t\n",
    "\\endmatrix $$ \n",
    "\n",
    "Furthermore, we know that $S = t - \\frac{1}{t} \\twonorm{\\x}^2 \\geq 0$, and also that $A = tI \\succeq 0$ is positive semidefinite, $\\forall t \\geq 0$. By the Schar Complement Lemma, we then claim that $X \\succeq 0 $ (i.e. $X$ is positive semidefinite). \n",
    "\n",
    "Finally, we can decompose $X$ into a linear combination of matrices with coefficients $t, x_1, \\dots, x_n$: \n",
    "\n",
    "$$ X = t \\beginmatrix I & \\vec 0 \\\\ \\vec 0 \\trans & 1 \\endmatrix \n",
    "+ x_1 \\beginmatrix \\huge {0} & \\beginmatrix 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\endmatrix \\\\\n",
    "\\beginmatrix 1 & 0 & \\cdots & 0 \\endmatrix & \n",
    "0\n",
    "\\endmatrix\n",
    "+ x_2 \\beginmatrix \\huge {0} & \\beginmatrix 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\endmatrix \\\\\n",
    "\\beginmatrix 0 & 1 & \\cdots & 0 \\endmatrix & \n",
    "0\n",
    "\\endmatrix\n",
    "+ \\dots\n",
    "+ x_n \\beginmatrix \\huge {0} & \\beginmatrix 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\endmatrix \\\\\n",
    "\\beginmatrix 0 & 0 & \\cdots & 1 \\endmatrix & \n",
    "0\n",
    "\\endmatrix\n",
    "\\succeq 0\n",
    "$$\n",
    "\n",
    "This, then, is clearly a Linear Matrix Inequality of the form \n",
    "\n",
    "$$t F_0 + x_1 F_1 + \\dots + x_n F_n \\succeq 0$$\n",
    "\n",
    "Therefore we have converted $\\twonorm {\\x} \\leq t $ to a linear matrix inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Show that $\\twonorm{A_i \\x + \\vec b_i} \\;\\leq\\; \\vec c_i\\trans \\x + d_i$ is a SDP constraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have shown that $\\twonorm\\y \\leq t$ is equivilant to $ F_0 + x_1 F_1 + \\dots + x_n F_n \\succeq 0$, so we an plug in $\\y = A\\x + \\vec b$ and $t = \\vec c \\trans \\x + d$. We get\n",
    " \n",
    "$$ \n",
    "\\beginalign\n",
    "(\\vec c \\trans \\x + d) F_0 + ((A \\x)_1 + b_1) F_1\n",
    "+ \\dots + ((A \\x)_n + b_n) F_n &\\succeq 0 \\\\\n",
    "c_1 x_1 F_0 + \\dots + c_n x_n F_n + d F_0 \n",
    "+ a_1 \\x_1 F_1 + b_1 F_1 + \\dots + a_n \\x_n F_n + b_n F_n &\\succeq 0 \\\\\n",
    "(d F_0 + b_1 F_1 + \\dots + b_n F_n ) + x_1 (c_1 F_0 + a_1 F_1) + \\dots + x_n (c_n F_n + a_n F_n) & \\succeq 0 \\\\\n",
    "G_0 + x_1 G_1 + \\dots + x_n G_n & \\succeq 0\n",
    "\\endalign $$\n",
    "\n",
    "Therefore $\\twonorm{A_i \\x + \\vec b_i} \\;\\leq\\; \\vec c_i\\trans \\x + d_i$ is a SDP constraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## (c) [30 points] GP as convex optimization problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The standard form of GP, given by\n",
    "\n",
    "$$\n",
    "\\renewcommand \\exp[1]{\\text{exp}(#1)}\n",
    "\\minimize{\\x \\in\\positivereals^n}f_{0}(x)\\\\\n",
    "\\text{subject to}\\quad f_{i}(x) \\leq 1, \\quad i=1,...,m, \\\\\n",
    "\\qquad\\qquad h_{j}(x) = 1, \\quad j=1,...,p,$$\n",
    "\n",
    "where $f_{0}, f_{1}, ..., f_{m}$ are posynomials, and $h_{1}, ..., h_{p}$ are monomials, does not look like a convex optimization problem. But we can transform it to a convex optimization problem, by logarithmic change-of-variable $y_{i} = \\log x_{i}$ (so $x_{i} = \\exp{y_i}$), followed by applying $\\log(.)$ to the objective function and to the both sides of the constraints:\n",
    "\n",
    "$$ \\minimize{y\\in\\realsn} \\log(f_{0}(\\exp{y}))\\\\\n",
    "\\subjectto \\log(f_{i}(\\exp{y})) \\leq 0, \\quad i=1,...,m, \\\\\n",
    "\\qquad\\qquad \\log(h_{j}(\\exp{\\y})) = 0, \\quad j=1,...,p,$$\n",
    "\n",
    "where $\\exp{y}$ denotes elementwise exponential of the vector $y\\in\\mathbb{R}^{n}$. Clearly, the above and the original problems are equivalent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### (c.1) (10 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To understand why the transformed problem is convex, consider the simple case: $m=p=1$, and that\n",
    "\n",
    "$$f_{0}(\\x) =  \\sum_{k=1}^{K_{0}}\\alpha_{k}x_{1}^{\\beta_{1,k}}...x_{n}^{\\beta_{n,k}}, \\qquad\n",
    "f_{1}(x) =\n",
    " \\sum_{k=1}^{K_{1}}a_{k}x_{1}^{b_{1,k}}...x_{n}^{b_{n,k}}, \\qquad \n",
    "h_{1}(x) = c x_{1}^{d_{1}} x_{2}^{d_{2}} ... x_{n}^{d_{n}}, \\qquad \n",
    "\\alpha_{k}, a_{k}, c> 0.$$\n",
    "\n",
    "Prove that the transformed problem has the form \n",
    "\n",
    "$$\\minimize{y\\in\\realsn} \\log\\left(\\displaystyle\\sum_{k=1}^{K_{0}}\\exp{\\vec p_{k}\\trans \\y + q_{k}}\\right)\\\\\n",
    "\\subjectto\n",
    "\\log\\left(\\displaystyle\\sum_{k=1}^{K_{1}}\\exp{\\vec r_{k}\\trans \\y + s_{k}}\\right) \\leq 0,\\\\\n",
    "\\quad \\u^{\\top}\\y = t.$$\n",
    "\n",
    "In other words, derive the transformed problem data $p_{k}, q_{k}, r_{k}, s_{k}, u, t$ as function of the original problem data: $\\alpha_{k}, \\beta_{1,k}, ..., \\beta_{n,k}, a_{k}, b_{1,k}, ..., b_{n,k}, c, d_{1}, ..., d_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's begin by examining the inside of the summation, we'll define \n",
    "\n",
    "$$\n",
    "g(\\x) = \\alpha_{k}x_{1}^{\\beta_{1,k}}...x_{n}^{\\beta_{n,k}}\n",
    "$$\n",
    "\n",
    "This gives us\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(\\exp{\\y}) \n",
    "&= \\alpha_{k}\\exp{y_{1}}^{\\beta_{1,k}}...\\exp{y_{n}}^{\\beta_{n,k}} \\\\\n",
    "&= \\exp{q_k}\\exp{\\beta_{1,k} y_1}...\\exp{\\beta_{n,k} y_{n}} &&( \\text{let } a_k = e^{q_k}) \\\\\n",
    "&= \\exp{q_k + \\beta_{1,k} y_1 + \\cdots + \\beta_{n,k} y_n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can now take $\\vec p\\trans = \\beginmatrix \\beta_{1, k} & \\cdots & \\beta_{n, k} \\endmatrix$, so the above expression reduces to \n",
    "\n",
    "$$g(\\exp{\\y}) = \\exp{q_k + \\vec p \\trans \\y}$$\n",
    "\n",
    "Now, we plug into the summation.\n",
    "    \n",
    "$$ f_{0}(\\exp{\\y}) = \\sum_{k=1}^{K_{0}}g(\\exp{\\y}) = \\sum_{k=1}^{K_{0}} \\exp{q_k + \\vec p \\trans \\y}$$\n",
    "\n",
    "Therefore \n",
    "\n",
    "$$\\log(f_0(\\exp{\\y}) = \\log\\left(\\sum_{k=1}^{K_{0}} \\exp{q_k + \\vec p \\trans \\y}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "By the same process, we get\n",
    "\n",
    "$$\\log(f_{1}(\\exp{\\x})) =\n",
    " \\log\\left(\\sum_{k=1}^{K_{1}}\\exp{\\vec r_k\\trans \\y + s_{k}} \\right)\n",
    " $$\n",
    "\n",
    "if we let $a_k = \\exp{s_k}$ and $\\vec r \\trans = \\begin{bmatrix} b_{1,k} & \\cdots & b_{n,k}\\end{bmatrix}$.\n",
    "\n",
    "Therefore the inequality $f_1(\\x) \\leq 1$ transforms into \n",
    "\n",
    "$$ \\beginalign \n",
    "&\\log(f_1(\\exp{\\y}) \\leq \\log(1) \\\\\n",
    "&\\log\\left(\\sum_{k=1}^{K_{1}}\\exp{\\vec r_k\\trans \\y + s_{k}} \\right) \\leq 0\n",
    "\\endalign\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, for the equality conditions:\n",
    "\n",
    "$$\\beginalign\n",
    "h_1(x) &= c x_1^{d_1}... x_n^{d_n} \\\\\n",
    "h_1(\\exp{\\y}) &= c\\, \\exp{y_1}^{d_1}... \\exp{y_n}^{d_n} \\\\\n",
    "&= \\exp{-t}\\exp{d_1 y_1}... \\exp{d_n y_n} &&( \\text{let } c = e^{-t}) \\\\\n",
    "&= \\exp{d_1 y_1  + ... + d_n y_n - t} \\\\\n",
    "&= \\exp{\\u\\trans \\y - t} \\\\\n",
    "\\endalign$$\n",
    "\n",
    "Which gives us\n",
    "\n",
    "$$\\log(h_1(\\exp{\\y})) = \\u\\trans \\y - t $$\n",
    "\n",
    "Therefore $h_1(\\x) = 1 $ transforms into\n",
    "\n",
    "$$\n",
    "\\beginalign\n",
    "\\log(h_1(\\exp{\\y}))  &= \\log(1) \\\\ \n",
    "\\u\\trans \\y - t &= 0 \\\\\n",
    "\\u\\trans \\y &= t\n",
    "\\endalign\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### (c.2) (20 points)\n",
    "Prove that the transformed problem derived in part (c.1) is indeed a convex optimization problem.\n",
    "\n",
    "(Hint: First show that log-sum-exp is a convex function using second order condition. Then think operations preserving function convexity.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\newcommand {\\z}{\\vec z}$\n",
    "Let $g(\\x)$ be the log-sum-exp function\n",
    "\n",
    "$$g(\\x) = \\log(e^{x_1} + \\cdots + e^{x_n})$$\n",
    "\n",
    "To show that $g$ is convex, we find the Hessian $\\hessian g(\\x)$. To simplify, we will use $z_i = e^{x_i}$. First, we find the first partial derivative with respect to an arbitrary element of $\\x$:\n",
    "\n",
    "$\\newcommand {\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand {\\secondpp}[2]{\\frac{\\partial #1}{\\partial #2}}$\n",
    "$$ \\pp{g}{x_j} = \\frac{z_j}{\\onevec\\trans\\z}$$\n",
    "\n",
    "Then we take a second partial derivative with another arbitrary element of $\\x$:\n",
    "\n",
    "$$\\beginalign\n",
    "\\pp{^2g}{x_i\\partial x_j} \n",
    "&= \\pp{}{x_i}\\left(\\frac{z_i}{\\onevec\\trans \\z}\\right) \\\\\n",
    "&= \\pp{}{x_i}\\left(\\frac{1}{\\onevec\\trans\\z}\\right)z_j + \\frac{1}{\\onevec\\trans\\z}\\pp{z_i}{x_i} \\\\\n",
    "&= -\\frac{z_i z_j}{(\\onevec\\trans\\z)^2} + \\frac{\\delta_{ij} z_i}{\\onevec\\trans\\z} \\\\\n",
    "&= \\frac{\\delta_{ij} z_i}{\\onevec\\trans\\z} - \\frac{z_i z_j}{(\\onevec\\trans\\z)^2} \\\\\n",
    "&= \\left(\\frac{\\text{diag}(\\z)}{\\onevec\\trans \\z} -  \\frac{\\z \\z\\trans}{(\\onevec\\trans\\z)^2}\\right)_{ij}\n",
    "\\endalign\n",
    "$$\n",
    "\n",
    "Therefore \n",
    "\n",
    "$$\\hessian g(\\x) = \\frac{\\text{diag}(\\z)}{\\onevec\\trans \\z} -  \\frac{\\z \\z\\trans}{(\\onevec\\trans\\z)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we must show that $\\hessian g \\succeq 0$. \n",
    "\n",
    "$$\\beginalign \n",
    "\\v\\trans \\hessian g(\\x) \\v \n",
    "&= \\frac\n",
    "    {\\v\\trans \\text{diag}(\\z)\\v }\n",
    "    {\\onevec\\trans \\z}\n",
    "- \\frac\n",
    "    {\\v\\trans \\z \\z\\trans \\v}\n",
    "    {(\\onevec\\trans\\z)^2} \\\\\n",
    "&=\n",
    "\\frac {1}{(\\onevec\\trans \\z)^2} \\left(\n",
    "\\left(\\onevec\\trans \\z \\right) \\left(\\v\\trans \\text{diag}(\\z)\\v \\right)\n",
    "-\n",
    "\\v\\trans \\z \\z\\trans \\v \n",
    "\\right) \\\\\n",
    "&= \n",
    "\\frac{1}{(\\onevec\\trans \\z)^2} \\left(\n",
    "        \\left(\\sum_i^n z_i \\right)\n",
    "        \\left(\\sum_{i=1}^{n}v_i^2 z_i \\right) \n",
    "- \\sum_{i=1}^{n}\\sum_{j=1}^{n}v_i z_i v_j z_j\n",
    "    \\right) \\\\\n",
    "&= \n",
    "\\frac{1}{(\\onevec\\trans \\z)^2} \\left(\n",
    "        \\left(\\sum_i^n z_i \\right)\n",
    "        \\left(\\sum_{i=1}^{n}v_i^2 z_i \\right) \n",
    "- \\left(\\sum_{i=1}^{n}v_i z_i \\right)^2\n",
    "    \\right) \\\\\n",
    "&= \n",
    "\\frac{1}{(\\onevec\\trans \\z)^2} \n",
    "    \\left(\n",
    "          (\\vec a \\trans \\vec a)(\\vec b \\trans \\vec b) \n",
    "        - (\\vec a \\trans \\vec b)\n",
    "    \\right)\n",
    "\\endalign$$\n",
    "\n",
    "Where $\\vec a$ and $\\vec b$ are vectors with components \n",
    "\n",
    "$$a_i = \\sqrt{z_i}, \\quad b_i = v_i\n",
    "\\sqrt{z_i}$$\n",
    "\n",
    "By the Cauchy-Schwarz inequality, $(\\vec a \\trans \\vec a)(\\vec b \\trans \\vec b) \\geq (\\vec a \\trans \\vec b)$, so \n",
    "\n",
    "$$ \\v\\trans \\hessian g(\\x)\\v \\geq 0, \\quad \\forall \\v \\in \\realsn,$$\n",
    "\n",
    "therefore $\\hessian g(\\x)$ is positive semi-definite and $g(\\x)$ is convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can see that the transformed objective function is in the form \n",
    "\n",
    "$$\\log\\left(\\displaystyle\\sum_{k=1}^{K_{0}}\\exp{\\vec p_{k}\\trans \\y + q_{k}}\\right)$$\n",
    "\n",
    "Let us take $x_i = \\vec p_i\\trans\\y + q_i$. We can put this into matrix form, giving us an affine function of $\\y$:\n",
    "\n",
    "$$ \\x = \\beginmatrix \\vec p_1\\trans \\\\ \\vdots \\\\ \\vec p_n\\trans \\endmatrix \\y+ \\vec q = P \\y + \\vec q$$\n",
    "\n",
    "The composition of an affine function preserves convexity, so \n",
    "\n",
    "$$ g(P\\y + \\vec q) = \\log(e^{\\vec p_1\\trans \\y + \\vec q_1} + \\cdots + e^{\\vec p_n\\trans \\y + \\vec q_n} )$$\n",
    "\n",
    "is convex, thus proving the convexity of the transformed objective function $\\log(f_0(\\exp{\\y})$. The form of the function in the inequality constraint $\\log(f_1(\\exp{\\y}) \\leq 0 $ is identical, therefore they it is also convex. \n",
    "\n",
    "The transformed equality constrain $\\u\\trans \\y = t$ is linear, so it is also convex, therefore the transformed problem is a convex optimization problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
